---
title: "DA5030 Final Project: Heart Disease Prediction"
subtitle: "Classification Modeling Using Logistic Regression, Decision Tree, Random Forest & Ensemble Methods"
author: "Haripal Deora"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r}
rm (list = ls())
```

```{r}
knitr::opts_chunk$set(echo = TRUE , warning = FALSE)

knitr::opts_chunk$set(
  echo    = TRUE,
  warning = FALSE,
  dev     = "svg" 
)

```

# Part 1: Introduction
The goal of this project is to build a predictive model for heart disease using the synthetic heart disease dataset. The dataset contains 50,000 patient records with demographic, lifestyle, and clinical features. The target variable is `Heart_Disease`, which makes this a binary classification problem.

The objective is to explore the data, prepare it for modeling, and compare multiple machine learning algorithms including Logistic Regression, Decision Tree, and Random Forest. The final step is to build an ensemble model to evaluate whether combining models improves predictive performance.

This project follows the CRISP-DM framework by organizing the workflow into data understanding, data preparation, modeling, evaluation, and final interpretation.

  
# Part 2: Data Acquisition & Initial Overview
```{r}




url<- "https://raw.githubusercontent.com/haripalsingh789-design/Final-project-da5030/refs/heads/main/synthetic_heart_disease_dataset.csv"
#Loading CSV file from  URL
df <- read.csv(url, stringsAsFactors = FALSE, header = TRUE)
# Check dimensions of the dataset
dim(df)



```
The dataset contains `r nrow(df)` observations and `r ncol(df)` variables. 
The initial structure confirms that the dataset loaded successfully from the CSV file. 



# Part 3: DATA EXPLORATION (EDA)
```{r}
# Inspect the dataset structure
str(df)
```

The structure of the dataset was examined using `str(df)`.  
Several variables are numeric (e.g., Age, Weight, Height, BMI, Cholesterol_Total, Systolic_BP),  
while others are categorical stored as character strings  
(e.g., Gender, Smoking, Alcohol_Intake, Physical_Activity, Diet, Stress_Level).
Binary clinical indicators such as Hypertension, Diabetes, Hyperlipidemia,  
Family_History, and Previous_Heart_Attack are stored as integers with values 0 and 1.
The target variable **Heart_Disease** is also an integer with values 0 or 1,  
confirming that this is a binary classification problem.

```{r}
# Summary Statistics
summary(df)
```
The `summary()` output gives a quick overview of all variables in the dataset.  
For the numeric variables (like Age, Weight, Height, BMI, blood pressure, heart rate, etc.) we can see the minimum, maximum, median and mean values. This helps us see the general ranges and whether values look reasonable.  
For the categorical variables (Gender, Smoking, Alcohol_Intake, Physical_Activity, Diet, Stress_Level), the summary shows that they are stored as character data and confirms that all levels are present as expected.


```{r}
# Check for Missing Values

# Count missing values per variable
colSums(is.na(df))

# Total missing values
sum(is.na(df))

```
Missing values were checked using `colSums(is.na(df))` and `sum(is.na(df))`.  
The results show **0 missing values** in every column, so we do not need to do any imputation. The dataset is complete and ready to use for further analysis and model building.



```{r}
# Distribution of Numerical Features
numeric_vars <- df[sapply(df, is.numeric)]

# Histogram for all numeric variables
par(mfrow = c(3, 4)) 
for(i in 1:ncol(numeric_vars)){
  hist(numeric_vars[, i], 
       main = paste("Histogram of", colnames(numeric_vars)[i]), 
       xlab = colnames(numeric_vars)[i], 
       col = "skyblue")
}
par(mfrow = c(1,1))

```

Histograms were created for all numeric variables (Age, Weight, Height, BMI, blood pressures, heart rate, blood sugar, cholesterol and the binary indicators).  
Most continuous variables appear roughly spread across their ranges without extreme spikes, while the binary clinical indicators (Hypertension, Diabetes, Hyperlipidemia, Family_History, Previous_Heart_Attack and Heart_Disease) show the expected two bars at 0 and 1. These plots give a first visual sense of how the data is distributed.


```{r}
# Barplots for Categorical Variables
cat_vars <- df[sapply(df, is.character)]

par(mfrow = c(3, 3))
for(i in 1:ncol(cat_vars)){
  barplot(table(cat_vars[, i]),
          main = paste("Barplot of", colnames(cat_vars)[i]),
          col = "lightgreen",
          las = 2)
}
par(mfrow = c(1,1))

```

Barplots were generated for the categorical variables: Gender, Smoking, Alcohol_Intake, Physical_Activity, Diet and Stress_Level.  
The plots show how many people fall into each category. For example, we can see the counts for Male vs Female, different smoking statuses (Current, Former, Never), levels of alcohol intake and activity, and different diet and stress groups. This helps us see whether the categories are balanced or dominated by a few levels.

```{r}
# Boxplots for key numeric variables
boxplot(df$Age,
        df$BMI,
        df$Weight,
        df$Height,
        names = c("Age", "BMI", "Weight", "Height"),
        main = "Boxplots of Key Numeric Features",
        col = "lightblue")

```
Boxplots are used to check the spread of the numeric features and to spot any outliers.
This helps me understand which variables might have extreme values that can affect modeling.



```{r}
# Chi-Square Test for Categorical Variables vs Target
cat_cols <- names(df)[sapply(df, is.character)]

chi_results <- list()

for(col in cat_cols){
  tbl <- table(df[[col]], df$Heart_Disease)
  chi_results[[col]] <- chisq.test(tbl)
}

chi_results

```

Chi-square tests were run between each categorical predictor and the target variable `Heart_Disease`. 
The output shows the test statistics and p-values for each variable. These tests give an initial idea of whether there is an association between each categorical feature (like Gender, Smoking, Alcohol_Intake, Physical_Activity, Diet and Stress_Level) and the presence of heart disease.

```{r}
# Outlier Detection for Numeric Variables
numeric_vars <- df[sapply(df, is.numeric)]

outlier_counts <- sapply(numeric_vars, function(x){
  sum(x < (quantile(x, 0.25) - 1.5*IQR(x)) |
      x > (quantile(x, 0.75) + 1.5*IQR(x)))
})

outlier_counts

```
Outliers in numeric variables were checked using the IQR rule (values beyond 1.5 × IQR from the first or third quartile).  
The counts show that most variables have very few or no outliers, but some indicators such as Diabetes and Previous_Heart_Attack have larger counts. This suggests that a number of patients fall into the “1” class for these variables, which is expected since they are binary. Overall, we do not see extreme outliers that would force immediate removal, but this check confirms how the values are spread.


```{r}
# Distribution Evaluation (Skewness/Kurtosis)
library(e1071)

skewness_values <- sapply(numeric_vars, skewness)
kurt_values <- sapply(numeric_vars, kurtosis)

skewness_values
kurt_values

```
Skewness and kurtosis were calculated for all numeric variables to understand the shape of their distributions.  
Most variables have skewness values close to zero and moderate kurtosis, suggesting roughly symmetric distributions without very heavy tails. A few variables show higher skewness or kurtosis, which means they are more tilted or have slightly heavier tails, but none appear severely distorted. This information will be useful later when deciding whether any transformations are needed for specific models.

```{r}
# PCA on numeric variables (for exploration only)
numeric_vars <- df[sapply(df, is.numeric)]

# Standardize numeric variables before PCA
numeric_scaled <- scale(numeric_vars)

pca_model <- prcomp(numeric_scaled, center = TRUE, scale. = TRUE)

# Proportion of variance explained
summary(pca_model)

# Scree plot
plot(pca_model, type = "l", main = "PCA Scree Plot – Numeric Features")

```
Principal Components Analysis (PCA) was applied to the standardized numeric features to understand the overall structure of the data. The variance explained plot shows how many components would be needed to capture most of the variability. PCA is used here as an exploratory tool and is not directly fed into the final models, but it satisfies the rubric requirement for identifying principal components.

```{r}
# Feature engineering: create an Age_Group variable
df$Age_Group <- cut(
  df$Age,
  breaks = c(0, 40, 60, 120),
  labels = c("Young", "Middle_Aged", "Senior"),
  right = FALSE
)

table(df$Age_Group)

```
A new derived feature Age_Group was created by binning the continuous Age variable into three categories: Young (0–39), Middle_Aged (40–59) and Senior (60+). This feature can help tree-based models capture age-related risk in a more interpretable way and satisfies the rubric requirement for feature engineering.

# Part 4: DATA PREPARATION

##  Dataset 1 : Logistic Regression
```{r}
#Create working copy for Logistic Regression
heart_logit <- df

# Categorical predictor columns
cat_cols <- c("Gender",
              "Smoking",
              "Alcohol_Intake",
              "Physical_Activity",
              "Diet",
              "Stress_Level")

# Convert categorical predictors to factors
heart_logit[cat_cols] <- lapply(heart_logit[cat_cols], as.factor)

```
A working copy of the original dataset is created and stored as `heart_logit`.  
The main categorical predictors (Gender, Smoking, Alcohol_Intake, Physical_Activity, Diet and Stress_Level) are converted to factors. The target `Heart_Disease` is kept as a numeric 0/1 variable, which is suitable for fitting a logistic regression model.


```{r}
# Cap outliers for key numeric variables

# Columns where we want to cap extreme values
cap_cols <- c("BMI",
              "Systolic_BP",
              "Diastolic_BP",
              "Blood_Sugar_Fasting",
              "Cholesterol_Total")

# Winsorize at 1st and 99th percentiles
for(col in cap_cols){
  q1  <- quantile(heart_logit[[col]], 0.01)
  q99 <- quantile(heart_logit[[col]], 0.99)
  heart_logit[[col]] <- pmax(pmin(heart_logit[[col]], q99), q1)
}

```
For logistic regression, very extreme values can have too much influence on the model.  
For BMI, systolic and diastolic blood pressure, fasting blood sugar and total cholesterol, values below the 1st percentile or above the 99th percentile are capped to those cut–offs. This keeps the overall range realistic while still keeping all records in the dataset.

```{r}
# Dummy encoding for logistic regression (full rank)
library(caret)

# Separate predictors and target
predictors_logit <- heart_logit[, setdiff(names(heart_logit), "Heart_Disease")]
target_logit     <- heart_logit$Heart_Disease

# Build dummy model for predictors ONLY, using fullRank = TRUE
dummy_model_logit <- dummyVars(~ ., data = predictors_logit, fullRank = TRUE)

# Create dummy-encoded predictors
logit_X <- data.frame(predict(dummy_model_logit, newdata = predictors_logit))

# Combine with target
heart_logit_dummy <- cbind(logit_X, Heart_Disease = target_logit)


```
Using `caret::dummyVars`, all factor predictors in `heart_logit` are converted into dummy (0/1) variables.  
The numeric variables remain numeric. The encoded predictors are stored in `heart_logit_dummy`, and the `Heart_Disease` outcome is added back as a separate factor column. This gives us a purely numeric predictor matrix suitable for logistic regression.


```{r}
# Standardize numeric predictor variables
numeric_cols_logit <- sapply(heart_logit_dummy, is.numeric)
numeric_cols_logit["Heart_Disease"] <- FALSE 

heart_logit_scaled <- heart_logit_dummy
heart_logit_scaled[, numeric_cols_logit] <- scale(heart_logit_dummy[, numeric_cols_logit])


```
All numeric predictor columns in `heart_logit_dummy` are standardized using `scale()`.  
Each predictor now has mean 0 and standard deviation 1, which helps the logistic regression optimization work more smoothly and prevents variables with large scales from dominating the model. The scaled dataset is saved as `heart_logit_scaled`.

```{r}
# Check multicollinearity using VIF
library(car)

logit_temp_model <- glm(Heart_Disease ~ ., 
                        data   = heart_logit_scaled,
                        family = binomial)

vif_values <- vif(logit_temp_model)
vif_values


```
Before using the dataset for Logistic Regression, we check multicollinearity using Variance Inflation Factor (VIF). A high VIF means that predictors are strongly correlated with each other, which can affect model stability. After dummy encoding and scaling, we fit a temporary logistic regression model and compute VIF values for all predictors. This helps us confirm that no variables are highly collinear.

```{r}
# Final logistic regression dataset
heart_logit_final <- heart_logit_scaled

```

## Dataset 2 : Decision Tree
```{r}
# Create dataset for Decision Tree
heart_tree <- df

# Convert categorical variables to factors
categorical_cols <- c("Gender", "Smoking", "Alcohol_Intake", "Physical_Activity",
                      "Diet", "Stress_Level", "Hypertension", "Diabetes",
                      "Hyperlipidemia", "Family_History", "Previous_Heart_Attack",
                      "Heart_Disease")

heart_tree[categorical_cols] <- lapply(heart_tree[categorical_cols], factor)


```
Decision Trees work directly with categorical features when they are stored as factors. Here we convert all character and binary variables into factors so the model can properly split on them.

```{r}
# Final Decision Tree dataset
heart_tree_final <- heart_tree
```
The dataset for the Decision Tree model is now ready. All categorical variables are stored as factors, and numeric values remain unchanged. This version will later be used for train-test splitting and model training.



##  Dataset 3 : Random Forest
```{r}
# Create dataset for Random Forest
heart_rf <- df

# Categorical and binary columns
rf_cat_cols <- c("Gender", "Smoking", "Alcohol_Intake", "Physical_Activity",
                 "Diet", "Stress_Level", "Hypertension", "Diabetes",
                 "Hyperlipidemia", "Family_History", "Previous_Heart_Attack",
                 "Heart_Disease")

heart_rf[rf_cat_cols] <- lapply(heart_rf[rf_cat_cols], factor)

```
Random Forest handles categorical data when it is stored as factors.  
Here we convert all character variables and binary clinical indicators (0/1) into factors, including the target `Heart_Disease`. This prepares the data for classification with Random Forest.

```{r}

# Final Random Forest dataset
heart_rf_final <- heart_rf

```
The Random Forest dataset keeps all numeric variables in their original scale and all categorical and binary variables as factors. Outliers are not removed, since Random Forest is robust to them. This finalized version, `heart_rf_final`, will be used later for train-test splitting and training the Random Forest model.

# Part 5: DATA SPLITTING

## Train–Test Split 
```{r}
library(caret)

set.seed(123)

# One common index based on the original outcome
master_index <- createDataPartition(df$Heart_Disease,
                                    p = 0.7,
                                    list = FALSE)

# Logistic Regression split
logit_train <- heart_logit_final[master_index, ]
logit_test  <- heart_logit_final[-master_index, ]

# Decision Tree split
tree_train <- heart_tree_final[master_index, ]
tree_test  <- heart_tree_final[-master_index, ]

# Random Forest split
rf_train <- heart_rf_final[master_index, ]
rf_test  <- heart_rf_final[-master_index, ]



```
The same **70/30 train–test split (using master_index)** is applied to all three modeling datasets so that logistic regression, decision tree, and random forest are trained and evaluated on exactly the same observations.


# Part 6: MODEL BUILDING

## Logistic Regression Model
```{r}
# Full logistic regression model
logit_model <- glm(Heart_Disease ~ ., 
                   data = logit_train,
                   family = binomial)

summary(logit_model)



```
The model was trained to predict heart disease using all variables. Some features came out very important:

-Age, Hypertension, Diabetes, Previous Heart Attack, and Cholesterol_Total are the strongest predictors.

-These have very small p-values, so they clearly affect heart disease risk.

-Lifestyle factors like Smoking, Diet, Physical Activity were not strong predictors in this dataset.

The model fit is good, the deviance went down a lot from null to residual, and it only took a few iterations to converge.
Overall, the model mainly depends on medical/clinical features. Older people, people with high BP, diabetes, or a past heart attack have a much higher chance of heart disease. Lifestyle variables didn’t show a big effect here.

```{r}
# Predictions on Test Set

logit_prob <- predict(logit_model,
                      newdata = logit_test,
                      type = "response")
logit_pred <- ifelse(logit_prob > 0.5, 1, 0)

logit_pred <- factor(logit_pred, levels = c(0,1))
logit_actual <- factor(logit_test$Heart_Disease, levels = c(0,1))

```


```{r}
# Confusion Matrix + Accuracy, Recall, Precision, F1

library(caret)

logit_cm <- confusionMatrix(logit_pred,
                            logit_actual,
                            positive = "1")

logit_cm

# Extract precision and recall from confusion matrix
logit_precision <- logit_cm$byClass["Pos Pred Value"]
logit_recall    <- logit_cm$byClass["Sensitivity"]

# Calculate F1 Score
logit_f1 <- 2 * (logit_precision * logit_recall) / (logit_precision + logit_recall)

logit_f1

```
The logistic regression model performed well on the test set.  
It achieved an accuracy of **`r round(logit_cm$overall["Accuracy"], 4)`**, which is much higher than the No Information Rate (**`r round(logit_cm$overall["AccuracyNull"], 4)`**).  
This means the model is clearly better than random guessing.

- The **Kappa score** is **`r round(logit_cm$overall["Kappa"], 3)`**, showing strong agreement between predictions and actual outcomes.  
- **Sensitivity (Recall)** is **`r round(logit_cm$byClass["Sensitivity"], 3)`**, meaning the model identifies heart disease cases well.  
- **Specificity** is **`r round(logit_cm$byClass["Specificity"], 3)`**, showing the model correctly identifies people without heart disease.  
- The **F1-score** is **`r round(logit_f1, 4)`**, confirming a strong balance between precision and recall.

Overall, the model shows **strong and consistent performance**, making reliable predictions for both classes.


```{r}
# ROC Curve + AUC

library(pROC)

logit_roc <- roc(logit_actual, logit_prob)
plot(logit_roc, col = "blue", lwd = 2)
auc(logit_roc)

```

The ROC curve shows how well the logistic regression model separates people with and without heart disease. The curve is very close to the top-left corner, which means the model is doing a great job at distinguishing the two classes.

The AUC value is **`r round(auc(logit_roc), 4)`**, which is extremely high.  
An AUC close to 1 means the model has excellent predictive power and is very reliable in ranking positive vs negative cases.

Overall, the ROC curve confirms that this logistic regression model performs very strongly.

## Decision Tree Model
```{r}
# Libraries
library(rpart)
library(rpart.plot)
library(caret)
library(pROC)

```

```{r}
# Train basic rpart model
set.seed(123)

tree_model <- rpart(
  Heart_Disease ~ .,
  data   = tree_train,
  method = "class",
  control = rpart.control(
    cp       = 0.01,  # initial CP, will tune with CV
    maxdepth = 10,    # max tree depth
    minsplit = 50     # minimum observations needed to attempt a split
  )
)

tree_model

```
The decision tree was built using `rpart` with cp = 0.01, max depth = 10, and minsplit = 50.  
The printed tree shows how the model splits the data based on the most important variables.

The first few splits are made using features like **Cholesterol_Total**, **Hypertension**, **Diabetes**, **Previous_Heart_Attack**, and **Age**, which means the tree considers these variables most useful for predicting heart disease.

Nodes marked with `*` are terminal nodes (final predictions). Each node displays:
- number of samples  
- predicted class (0 or 1)  
- probability of each class  

Overall, the tree structure looks correct and shows how the model is separating healthy vs. heart-disease cases step by step.


```{r}
# Hyperparameter tuning

# Cross-validation results in CP table
printcp(tree_model)

# Plot CP vs cross-validated error
plotcp(tree_model)

```

The CP table helps us understand how the tree performs at different levels of complexity.  
As the CP value decreases, the tree becomes larger and the cross-validated error (`xerror`) steadily goes down.  
This means the model learns more structure from the data and becomes more accurate up to a certain depth.

From the CP plot, we can see a clear pattern:
- The error drops sharply at first  
- Then slowly flattens  
- After a certain point, extra splits give almost **no improvement**

This helps us choose the best CP value for pruning — the one with the lowest cross-validated error (`r round(min(tree_model$cptable[,"xerror"]),4)`).

Overall, the CP table + plot confirm that tuning is working correctly and that the tree benefits from some depth, but extremely deep trees do not add useful accuracy.


```{r}


# Best CP based on lowest cross-validated error (xerror)
best_cp <- tree_model$cptable[
  which.min(tree_model$cptable[,"xerror"]),
  "CP"
]

best_cp

```
```{r}
# Prune the tree using tuned CP
pruned_tree <- prune(tree_model, cp = best_cp)

# Visualize pruned tree
rpart.plot(pruned_tree, main = "Pruned Decision Tree – Heart Disease")

```

After selecting the best CP value, we pruned the tree to reduce overfitting and keep only the most important splits.  
The pruned tree now focuses on the strongest predictors and gives a cleaner, more interpretable structure.

The final model shows that:
- **Cholesterol_Total** is the first and most important split.
- **Hypertension**, **Age**, **Diabetes**, and **Previous_Heart_Attack** further separate high- and low-risk groups.
- The leaf nodes clearly show the predicted class along with class probabilities.

Overall, the pruned tree is simpler, easier to explain, and still captures the key patterns related to heart disease.



```{r}
# Predict on test set

# Predicted class labels
tree_pred_class <- predict(
  pruned_tree,
  newdata = tree_test,
  type = "class"
)

# Predicted probabilities for class "1" (needed for ROC/AUC)
tree_pred_prob <- predict(
  pruned_tree,
  newdata = tree_test,
  type = "prob"
)[, "1"]

# Actual labels
tree_actual <- tree_test$Heart_Disease

```


```{r}
# Confusion matrix + Accuracy + Precision + Recall + F1

tree_cm <- confusionMatrix(
  tree_pred_class,
  tree_actual,
  positive = "1"
)

tree_cm


tree_precision <- tree_cm$byClass["Precision"]
tree_recall    <- tree_cm$byClass["Sensitivity"]

tree_f1 <- 2 * (tree_precision * tree_recall) /
           (tree_precision + tree_recall)

tree_f1

```

The pruned decision tree achieved **perfect classification performance** on the test set.  
All evaluation metrics show a value of **1**, meaning the model correctly predicted every case with no errors.

- **Accuracy:** `r round(tree_cm$overall["Accuracy"], 4)`
- **Kappa:** `r round(tree_cm$overall["Kappa"], 3)`
- **Sensitivity (Recall):** `r round(tree_cm$byClass["Sensitivity"], 4)`
- **Specificity:** `r round(tree_cm$byClass["Specificity"], 4)`
- **Precision:** `r round(tree_cm$byClass["Precision"], 4)`
- **F1 Score:** `r round(tree_f1, 4)`

These results indicate that:

- The model correctly identifies **all true positive heart disease cases**.  
- It also correctly identifies **all true negative cases** with no misclassification.  
- A precision of 1 means the model does not produce any false positives.  
- A recall of 1 means the model does not miss any actual heart disease cases.  
- The F1 score confirms a perfect balance between precision and recall.

Although perfect performance is rare in real-world datasets, it can occur with **synthetic data**, where patterns are clean and highly separable.  
In this case, the decision tree captures the structure of the dataset extremely well and achieves flawless test-set predictions.



```{r}
# ROC curve + AUC

tree_roc <- roc(tree_actual, tree_pred_prob)

# Plot ROC curve
plot(
  tree_roc,
  col = "darkgreen",
  lwd = 2,
  main = "ROC Curve – Decision Tree"
)

# AUC value
auc(tree_roc)

```

The ROC curve for the decision tree shows a perfect separation between the two classes.  
The curve immediately rises to the top-left corner, which means the model is able to correctly rank all positive and negative cases without any overlap.

The AUC value is:
**AUC = `r round(auc(tree_roc), 4)`**
An AUC of **1** indicates perfect discriminatory ability.  
This means the decision tree correctly assigns higher probabilities to all true heart-disease cases than to non-cases.

Although perfect AUC is rare in real datasets, it can occur in **synthetic data**, where the patterns between predictors and the target are extremely clean. In this dataset, the tree was able to fully capture those patterns, resulting in perfect classification performance.




## Random Forest Model
```{r}
library(randomForest)
library(caret)
library(pROC)

set.seed(123)

# Train Random Forest model
rf_model <- randomForest(
  Heart_Disease ~ ., 
  data = rf_train,
  ntree = 500,                                  # number of trees
  mtry  = floor(sqrt(ncol(rf_train) - 1)),      # features per split (default rule)
  importance = TRUE
)

rf_model


```
The Random Forest model was trained using 500 trees, with 4 variables tried at each split (based on the default square-root rule). The training summary shows that the model perfectly classified all training observations, resulting in an OOB (Out-of-Bag) error rate of **0%**. The OOB confusion matrix confirms this, with no misclassifications for either class. This kind of perfect accuracy can happen in synthetic datasets like this one, where clinical variables such as cholesterol, hypertension, diabetes, previous heart attack, and age clearly separate the two classes. Overall, the model fits the training data extremely well and captures the patterns in this dataset with complete accuracy.


```{r}
# Plot variable importance
varImpPlot(rf_model, main = "Random Forest – Variable Importance")

```
The variable importance plot shows which predictors the Random Forest model relied on most when classifying heart disease. In this dataset, a few clinical features stand out clearly. Age is the strongest predictor, followed by hypertension, total cholesterol, diabetes, and previous heart attack history. These variables show the highest decreases in accuracy and Gini impurity when removed, meaning the model becomes noticeably worse without them.
Most lifestyle variables such as diet, stress level, smoking, and alcohol intake contribute very little to the model. This matches the behavior seen in the other models as well, where the clinical factors dominate. Overall, the plot confirms that heart disease in this synthetic dataset is driven mainly by medical history and biometric indicators rather than lifestyle categories.


```{r}
# Predictions on Test Set
rf_pred_class <- predict(rf_model, newdata = rf_test)

rf_pred_prob <- predict(
  rf_model,
  newdata = rf_test,
  type   = "prob"
)[, "1"]

rf_actual <- rf_test$Heart_Disease

```



```{r}
# Confusion Matrix + Metrics
rf_cm <- confusionMatrix(
  rf_pred_class,
  rf_actual,
  positive = "1"
)

rf_cm

rf_precision <- rf_cm$byClass["Precision"]
rf_recall    <- rf_cm$byClass["Sensitivity"]

rf_f1 <- 2 * (rf_precision * rf_recall) /
         (rf_precision + rf_recall)

rf_f1

```


The Random Forest model achieved **perfect classification performance** on the test set.  
All evaluation metrics show a value of **1**, meaning the model correctly predicted every case with zero errors.

- **Accuracy:** `r round(rf_cm$overall["Accuracy"], 4)`
- **Kappa:** `r round(rf_cm$overall["Kappa"], 4)`
- **Sensitivity (Recall):** `r round(rf_cm$byClass["Sensitivity"], 4)`
- **Specificity:** `r round(rf_cm$byClass["Specificity"], 4)`
- **Precision:** `r round(rf_cm$byClass["Precision"], 4)`
- **F1 Score:** `r round(rf_f1, 4)`

These results indicate that:

- The model correctly identifies **all true positive heart disease cases**.
- It also correctly identifies **all true negative cases** with **no misclassification**.
- A precision of 1 means the model **does not produce any false positives**.
- A recall of 1 means the model **does not miss any actual cases**.
- The F1-score confirms a **perfect balance** between precision and recall.

While such perfect performance is rare in real-world datasets, it can occur with **synthetic data**,  
where patterns are strong, clean, and highly separable.  
In this case, the Random Forest model captures the dataset structure extremely well  
and achieves flawless test-set predictions.


```{r}
# ROC Curve + AUC
rf_roc <- roc(rf_actual, rf_pred_prob)

plot(
  rf_roc,
  col = "red",
  lwd = 2,
  main = "ROC Curve – Random Forest"
)

auc(rf_roc)

```
The ROC curve for the Random Forest model shows **perfect classification performance** on the test set.  
The curve rises straight up to (0,1), indicating that the model is able to separate both classes with no overlap.

- **AUC:** `r round(auc(rf_roc), 4)`

This result means that:

- The model can correctly rank **all positive cases above negative cases**.  
- There are **no incorrect probability rankings** between the two classes.  
- The probability predictions perfectly distinguish heart disease from non–heart disease cases.

Although perfect AUC values are extremely rare in real-world datasets, this behavior makes sense for **synthetic data**, where patterns are very clean and highly separable.  
Here, the Random Forest captures those strong relationships and produces flawless probability predictions.

```{r}
library(caret)
library(randomForest)

set.seed(123)

# 5-fold cross-validation control
rf_cv_ctrl <- trainControl(
  method      = "cv",
  number      = 5,       # k = 5 folds (lighter than 10)
  verboseIter = FALSE
)

# Number of predictors (excluding target)
p <- ncol(rf_train) - 1

# Tuning grid for mtry
mtry_grid <- expand.grid(
  mtry = c(
    max(1, floor(sqrt(p) / 2)),
    floor(sqrt(p)),
    min(p, floor(sqrt(p) * 1.5))
  )
)

# Train Random Forest with 5-fold CV
rf_cv <- train(
  Heart_Disease ~ .,
  data      = rf_train,
  method    = "rf",
  trControl = rf_cv_ctrl,
  tuneGrid  = mtry_grid,
  ntree     = 200,
  metric    = "Accuracy"
)

rf_cv
plot(rf_cv, main = "5-fold CV Accuracy vs mtry (Random Forest)")

best_mtry <- rf_cv$bestTune$mtry
best_mtry

# Evaluate CV-tuned RF on test set

rf_cv_pred <- predict(rf_cv, newdata = rf_test)

rf_cv_conf <- confusionMatrix(
rf_cv_pred,
rf_test$Heart_Disease,
positive = "1"
)

rf_cv_conf

# Extract metrics

rf_cv_acc    <- rf_cv_conf$overall["Accuracy"]
rf_cv_sens   <- rf_cv_conf$byClass["Sensitivity"]
rf_cv_spec   <- rf_cv_conf$byClass["Specificity"]
rf_cv_prec   <- rf_cv_conf$byClass["Pos Pred Value"]
rf_cv_recall <- rf_cv_sens
rf_cv_F1     <- 2 * rf_cv_prec * rf_cv_recall / (rf_cv_prec + rf_cv_recall)

rf_cv_acc
rf_cv_F1
```

To satisfy the rubric requirement for k-fold cross-validation, a 5-fold CV strategy was applied to the random forest model using caret::train. A small grid of mtry values was evaluated, and the best value was selected based on cross-validated accuracy. This CV-tuned model was then evaluated on the held-out test set, and its accuracy and F1-score were compared to the original random forest model.

# Part 7: BAGGING

## Bagged Logistic Regression
```{r}
set.seed(123)

# Number of bootstrap models
B <- 50  

# Store all bootstrap logistic models
bag_logit_models <- vector("list", B)

# Fit B bootstrap logistic regression models
for (b in 1:B) {
  # Sample with replacement from the training data
  idx <- sample(1:nrow(logit_train), replace = TRUE)
  boot_data <- logit_train[idx, ]
  
  bag_logit_models[[b]] <- glm(
    Heart_Disease ~ .,
    data   = boot_data,
    family = binomial
  )
}


# Get predicted probabilities on the test set from each model
bag_prob_matrix <- sapply(bag_logit_models, function(m) {
  predict(m, newdata = logit_test, type = "response")
})

# Average probability across all bootstrap models
bag_prob <- rowMeans(bag_prob_matrix)

# Convert probabilities to class labels using 0.5 threshold
bag_pred <- ifelse(bag_prob >= 0.5, 1, 0)

bag_pred_factor   <- factor(bag_pred, levels = c(0, 1))
bag_actual_factor <- factor(logit_test$Heart_Disease, levels = c(0, 1))

# Confusion matrix and metrics
bag_cm <- confusionMatrix(
  bag_pred_factor,
  bag_actual_factor,
  positive = "1"
)

bag_cm

# Precision, recall, F1 score
bag_precision <- bag_cm$byClass["Precision"]
bag_recall    <- bag_cm$byClass["Sensitivity"]

bag_f1 <- 2 * (bag_precision * bag_recall) /
          (bag_precision + bag_recall)

bag_f1

# ROC curve and AUC for bagged logistic regression
bag_roc <- roc(bag_actual_factor, bag_prob)

plot(
  bag_roc,
  lwd  = 2,
  main = "ROC Curve – Bagged Logistic Regression"
)

auc(bag_roc)


```
The bagged logistic regression model was built by training 50 separate logistic regression models on bootstrap samples of the training data and averaging their predicted probabilities. Bagging helps reduce variance and makes the logistic regression model more stable.

The bagged model achieved an accuracy of **`r round(bag_cm$overall["Accuracy"], 4)`**, which is higher than the No Information Rate. The Kappa value is **`r round(bag_cm$overall["Kappa"], 4)`**, showing strong agreement between predicted and actual values.

Sensitivity is **`r round(bag_cm$byClass["Sensitivity"], 4)`**, meaning the model correctly identifies most true heart disease cases. Specificity is **`r round(bag_cm$byClass["Specificity"], 4)`**, showing that the model also correctly identifies non-disease cases. Precision is **`r round(bag_cm$byClass["Precision"], 4)`**, and the F1 score is **`r round(bag_f1, 4)`**, confirming a strong balance between precision and recall.

The ROC curve shows excellent separation between the two classes, and the AUC value is **`r round(auc(bag_roc), 4)`**, indicating the bagged model is highly effective at ranking positive and negative cases.

Overall, the performance of the bagged logistic regression model is very similar to the original single logistic regression model, with some metrics improving slightly due to the variance reduction from bagging.


**Bagging was applied only to logistic regression because the decision tree and random forest models already achieved 100 percent accuracy on this synthetic dataset, so additional ensemble methods were not needed for them.**

# Part 8: ENSEMBLE MODEL
```{r}
## Heterogeneous Ensemble Model – Logistic, Decision Tree, Random Forest

library(pROC)
library(caret)

# Logistic Regression probabilities 
ens_logit_prob <- logit_prob          

# Decision Tree probabilities 
ens_tree_prob  <- tree_pred_prob      

# Random Forest probabilities 
ens_rf_prob    <- rf_pred_prob        

# Optional check
length(ens_logit_prob); length(ens_tree_prob); length(ens_rf_prob)

# Weights based on F1-scores of each base model

sum_f1  <- as.numeric(logit_f1 + tree_f1 + rf_f1)

w_logit <- as.numeric(logit_f1 / sum_f1)
w_tree  <- as.numeric(tree_f1  / sum_f1)
w_rf    <- as.numeric(rf_f1    / sum_f1)

w_logit  # weight for logistic regression
w_tree   # weight for decision tree
w_rf     # weight for random forest

# 3. Weighted average of probabilities
ens_prob <- (w_logit * ens_logit_prob) +
            (w_tree  * ens_tree_prob)  +
            (w_rf    * ens_rf_prob)

# 4. Convert ensemble probabilities to class labels
ens_pred <- ifelse(ens_prob >= 0.5, 1, 0)
ens_pred <- factor(ens_pred, levels = c(0, 1))

# Use logistic test set as reference (same rows as others)
ens_actual <- factor(logit_test$Heart_Disease, levels = c(0, 1))

# 5. Confusion matrix and metrics for ensemble
ens_cm <- confusionMatrix(
  ens_pred,
  ens_actual,
  positive = "1"
)

ens_cm

ens_precision <- ens_cm$byClass["Precision"]
ens_recall    <- ens_cm$byClass["Sensitivity"]

ens_f1 <- 2 * (ens_precision * ens_recall) /
          (ens_precision + ens_recall)

ens_f1

# 6. ROC curve and AUC for ensemble model
ens_roc <- roc(ens_actual, ens_prob)

plot(
  ens_roc,
  lwd  = 2,
  main = "ROC Curve – Ensemble Model"
)

auc(ens_roc)


```
The final ensemble model combines the three base classifiers: logistic regression, the pruned decision tree and the random forest. For each model, predicted probabilities for the positive class are computed on the same test set. These probabilities are then combined using a weighted average. The weights are based on each model’s F1 score so that models with higher F1 receive slightly more influence in the ensemble. In this dataset the weights are approximately `r round(w_logit, 3)` for logistic regression, `r round(w_tree, 3)` for the decision tree and `r round(w_rf, 3)` for the random forest.

The ensemble achieves an accuracy of **`r round(ens_cm$overall["Accuracy"], 4)`**, which is essentially perfect and far above the No Information Rate. The Kappa value is **`r round(ens_cm$overall["Kappa"], 4)`**, showing almost perfect agreement between predicted and true labels. Sensitivity is **`r round(ens_cm$byClass["Sensitivity"], 4)`** and specificity is **`r round(ens_cm$byClass["Specificity"], 4)`**, meaning the ensemble correctly identifies almost all patients with heart disease as well as those without it. The positive predictive value is extremely high, and the overall F1 score is **`r round(ens_f1, 4)`**, indicating a nearly perfect balance between precision and recall.

The ROC curve for the ensemble rises straight to the top left corner, and the AUC is **`r round(auc(ens_roc), 4)`**, which corresponds to perfect class separation. This confirms that the ensemble ranks all true heart disease cases above the non–disease cases on the test set.

Overall, this heterogeneous ensemble effectively combines the strengths of logistic regression, the decision tree and the random forest. Because the tree based models already perform almost perfectly on this synthetic dataset, the ensemble naturally inherits this performance and becomes nearly perfect as well, while still illustrating how different algorithms can be blended into a single powerful classifier.


# Part 9: EVALUATION & INTERPRETATION
```{r}
# Summary table for all model metrics

model_names <- c("Logistic Regression",
"Decision Tree",
"Random Forest",
"Ensemble Model")

accuracy_values  <- c(logit_cm$overall["Accuracy"],
tree_cm$overall["Accuracy"],
rf_cm$overall["Accuracy"],
ens_cm$overall["Accuracy"])

precision_values <- c(logit_cm$byClass["Precision"],
tree_cm$byClass["Precision"],
rf_cm$byClass["Precision"],
ens_cm$byClass["Precision"])

recall_values    <- c(logit_cm$byClass["Sensitivity"],
tree_cm$byClass["Sensitivity"],
rf_cm$byClass["Sensitivity"],
ens_cm$byClass["Sensitivity"])

f1_values <- c(logit_f1,
tree_f1,
rf_f1,
ens_f1)

auc_values <- c(auc(logit_roc),
auc(tree_roc),
auc(rf_roc),
auc(ens_roc))

model_results <- data.frame(
Model     = model_names,
Accuracy  = round(accuracy_values, 4),
Precision = round(precision_values, 4),
Recall    = round(recall_values, 4),
F1_Score  = round(f1_values, 4),
AUC       = round(auc_values, 4)
)

model_results

```
### Interpretation of the Comparison Table

The comparison table shows a clear progression from a linear model to more flexible tree-based and ensemble models.

**Logistic Regression**  
- Accuracy: **`r round(logit_cm$overall["Accuracy"], 4)`**  
- F1-Score: **`r round(logit_f1, 4)`**  
- AUC: **`r round(auc(logit_roc), 4)`**  
Logistic Regression performs well but, because it assumes linear relationships, it cannot fully capture nonlinear or interaction effects in the data. This is why its performance is slightly lower than the tree-based models.

**Decision Tree**  
- Accuracy: **`r round(tree_cm$overall["Accuracy"], 4)`**  
- Precision: **`r round(tree_cm$byClass["Precision"], 4)`**  
- Recall: **`r round(tree_cm$byClass["Sensitivity"], 4)`**  
- AUC: **`r round(auc(tree_roc), 4)`**  
The tree learns clear and strong splits on key clinical features, so it reaches almost perfect classification on this synthetic dataset.

**Random Forest**  
- Accuracy: **`r round(rf_cm$overall["Accuracy"], 4)`**  
- Precision: **`r round(rf_cm$byClass["Precision"], 4)`**  
- Recall: **`r round(rf_cm$byClass["Sensitivity"], 4)`**  
- F1-Score: **`r round(rf_f1, 4)`**  
- AUC: **`r round(auc(rf_roc), 4)`**  
Random Forest achieves perfect performance across all metrics. By averaging many trees, it captures every important pattern and generalizes extremely well on this dataset.

**Ensemble Model (Logit + Tree + RF)**  
- Accuracy: **`r round(ens_cm$overall["Accuracy"], 4)`**  
- Precision: **`r round(ens_cm$byClass["Precision"], 4)`**  
- Recall: **`r round(ens_cm$byClass["Sensitivity"], 4)`**  
- F1-Score: **`r round(ens_f1, 4)`**  
- AUC: **`r round(auc(ens_roc), 4)`**  
The ensemble performs almost the same as the Random Forest. It gains stability by combining three strong base models while keeping near-perfect discrimination.

---

### Why the Ensemble Performed Best

Although the Random Forest is already extremely strong, combining predictions from **Logistic Regression**, **Decision Tree** and **Random Forest** produces a model that is very stable and balanced.

In the ensemble, probabilities are combined using **F1-score–based weights**, so models with higher F1 (Decision Tree and Random Forest) receive more influence, and Logistic Regression contributes mainly by smoothing borderline cases. Because the tree-based models have near-perfect F1-scores, they dominate the weighting, and the final ensemble also becomes nearly perfect.

---

### Feature-Level Interpretation

#### Logistic Regression — Coefficient Meaning

Logistic Regression estimates how each variable changes the log-odds of heart disease:

- **Age** has a positive coefficient, so risk increases with age.  
- **Hypertension, Diabetes and Hyperlipidemia** all have strong positive effects.  
- **Previous_Heart_Attack** is the strongest predictor in the model.  
- **Cholesterol_Total** also shows a clear positive effect on risk.  
- Lifestyle features (smoking, alcohol, diet, stress) have smaller effects in this synthetic dataset.

Overall, this model mainly highlights the core clinical risk factors for heart disease.

#### Decision Tree — Key Splits

The Decision Tree identified the following variables as most important in its splits:

- **Cholesterol_Total** (root split)  
- **Hypertension**  
- **Age**  
- **Diabetes**  
- **Previous_Heart_Attack**

The tree first divides patients based on cholesterol and then checks major risk conditions such as hypertension and diabetes. The clear and simple structure explains its near-perfect accuracy.

#### Random Forest — Feature Importance

The Random Forest importance plot consistently ranked the same top predictors:

- **Age**  
- **Hypertension**  
- **Cholesterol_Total**  
- **Diabetes**  
- **Previous_Heart_Attack**

Because Random Forest captures nonlinear relationships and interactions between these variables, it reaches perfect predictive performance on this dataset.

---

# Part 10: DEPLOYMENT / PRESENTATION
```{r}
# Take one example from the test set
example <- logit_test[1, ]

# Ensemble probability for that row
example_prob <- ens_prob[1]

# Ensemble class for that row
example_class <- ifelse(example_prob >= 0.5, 1, 0)

example_prob
example_class
example$Heart_Disease

```
For deployment, the final ensemble was applied to a single patient from the test set. The model outputs a predicted probability of heart disease (r round(example_prob, 4)) and a final class decision (r example_class). This demonstrates how the trained ensemble can be used in practice to score individual records, as required by the rubric.

### Final Interpretation and Conclusion

This project compared four models for heart-disease prediction: Logistic Regression, Decision Tree, Random Forest and a heterogeneous Ensemble. The results show that:

- Tree-based models outperform Logistic Regression because they capture nonlinear patterns and feature interactions.  
- Random Forest is the strongest individual model, with perfect performance across all metrics.  
- The Ensemble Model matches Random Forest and provides very stable probability predictions by combining all three approaches.  
- Logistic Regression remains useful for interpretability but is limited by its linear structure.

Across all models, key medical variables such as **cholesterol**, **hypertension**, **diabetes**, **age** and **previous heart attack** drive prediction performance, which agrees with clinical understanding of heart disease. The ensemble demonstrates how multiple algorithms can be combined into a single, highly accurate and reliable classifier for this heart-disease dataset.

## Final Reflection 

This project helped me understand the full CRISP-DM process, from data preparation to model deployment. One of the most important things I learned is how different algorithms behave on the same dataset. Logistic Regression gave a clear and interpretable baseline, but the tree-based models captured the nonlinear relationships much more effectively. Building the ensemble also taught me how combining models can increase stability and confidence in predictions.

What went well was the strong performance of all tree-based models and the smooth integration of the ensemble. The major challenge was working with a synthetic dataset, because the patterns were very clean and did not fully represent real-world medical data. This made model performance extremely high, which is good for learning but not realistic for clinical deployment. Another challenge was ensuring consistent preprocessing across all models, especially with dummy encoding and scaling.

If I had more time, I would extend this project by applying the same workflow to a real clinical dataset, exploring cross-validation, and adding more features such as lab test results, ECG data, and family history. I would also try techniques like SMOTE and cost-sensitive learning to improve generalization. Finally, deploying the ensemble as a small Shiny app or API would make it more useful in a real healthcare setting.




